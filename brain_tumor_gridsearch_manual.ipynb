{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification: Manual Gridsearch\n",
    "The data has already been preprocessed in the Setup & EDA notebook.  I took the hard route on this project, that being a manual grid search over hyperparameters for a model trained from scratch.  One great alternative to a manaul gridsearch is to *wrap the keras model in an sklearn wrapper object and use an sklearn GridSearch or RandomSearch object.*  The reason I'm skipping this option is two-fold:  \n",
    "1. this approach isn't compatible with Tensorboard and I wanted to explore my results with TensorBoard this time, \n",
    "2. To distrubute this workload across multiple instances, stop, start, and pick up another time, it's easiest to make a list of hyperparameter combinations and track your index in this list, or split the list up between instances such as those on Colab  \n",
    "\n",
    "The second part to this, the fact I'm training a model from scratch instead of applying transfer learning, is simply to see how well I can do for this project and then compare the results to what I can achieve with a basic transfer learning application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Brain-Tumor-Classification:-Manual-Gridsearch\" data-toc-modified-id=\"Brain-Tumor-Classification:-Manual-Gridsearch-1\">Brain Tumor Classification: Manual Gridsearch</a></span></li><li><span><a href=\"#import\" data-toc-modified-id=\"import-2\">import</a></span></li><li><span><a href=\"#paths\" data-toc-modified-id=\"paths-3\">paths</a></span></li><li><span><a href=\"#reproducibility-&amp;-maintenance\" data-toc-modified-id=\"reproducibility-&amp;-maintenance-4\">reproducibility &amp; maintenance</a></span></li><li><span><a href=\"#data-generators\" data-toc-modified-id=\"data-generators-5\">data generators</a></span></li><li><span><a href=\"#classes\" data-toc-modified-id=\"classes-6\">classes</a></span></li><li><span><a href=\"#gridsearch\" data-toc-modified-id=\"gridsearch-7\">gridsearch</a></span><ul class=\"toc-item\"><li><span><a href=\"#hyperparams\" data-toc-modified-id=\"hyperparams-7.1\">hyperparams</a></span></li><li><span><a href=\"#create_model()\" data-toc-modified-id=\"create_model()-7.2\">create_model()</a></span></li><li><span><a href=\"#train_test_model()\" data-toc-modified-id=\"train_test_model()-7.3\">train_test_model()</a></span></li><li><span><a href=\"#pickle_bm_lists()\" data-toc-modified-id=\"pickle_bm_lists()-7.4\">pickle_bm_lists()</a></span></li><li><span><a href=\"#run_benchmarking()\" data-toc-modified-id=\"run_benchmarking()-7.5\">run_benchmarking()</a></span></li><li><span><a href=\"#run-it!\" data-toc-modified-id=\"run-it!-7.6\">run it!</a></span></li></ul></li><li><span><a href=\"#Time-to-examine-the-results!\" data-toc-modified-id=\"Time-to-examine-the-results!-8\">Time to examine the results!</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tensorflow tools #\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (Dense, Input, MaxPooling2D, MaxPool2D, GlobalAveragePooling2D,\n",
    "                                     Conv2D, Flatten, Dropout, BatchNormalization) \n",
    "from tensorflow.keras.activations import sigmoid, softmax, relu\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# sklearn wrapper \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# ml viz\n",
    "from tensorboard.plugins.hparams import api as hp \n",
    "%load_ext tensorboard\n",
    "import pydot \n",
    "import graphviz \n",
    "# sklearn tools #\n",
    "import sklearn \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# image tools # \n",
    "from tensorflow.keras.preprocessing import image \n",
    "import cv2\n",
    "import imutils\n",
    "# data handling tools #\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# plotting tools #\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "# general tools #\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import os \n",
    "pjoin = os.path.join\n",
    "import shutil\n",
    "import gc\n",
    "import pickle \n",
    "import itertools\n",
    "import re \n",
    "import string \n",
    "# import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, val_path, test_path = './train', './val', './test'\n",
    "model_path = './model'\n",
    "bm_lists_path = './bm_lists'\n",
    "tb_log_path = './tb_runlog'\n",
    "\n",
    "path_ls = [train_path, val_path, test_path, model_path, \n",
    "           bm_lists_path, tb_log_path]\n",
    "\n",
    "for path in path_ls:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproducibility & maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(38)\n",
    "np.random.seed(38)\n",
    "_= gc.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train generator instantiation \n",
    "train_im_datagen = image.ImageDataGenerator(rescale=1./255, \n",
    "                                     rotation_range=.15, \n",
    "                                     width_shift_range=0, \n",
    "                                     height_shift_range=0, \n",
    "                                     brightness_range=(.1, .9),\n",
    "                                     shear_range=.15,\n",
    "                                     zoom_range=0,\n",
    "                                     horizontal_flip=True,\n",
    "                                     vertical_flip=True,\n",
    "                                     data_format='channels_last',\n",
    "                                     validation_split=0)\n",
    "# test generator instantiation\n",
    "test_im_datagen = image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_gens(batch_size):\n",
    "    ''' Given the batch size, returns a tuple of generators from the \n",
    "        designated train, val, and test paths:  (traingen, valgen, testgen). '''\n",
    "    # train data flow inititalized #\n",
    "    traingen = train_im_datagen.flow_from_directory(train_path, \n",
    "                                                    target_size=target_size, \n",
    "                                                    color_mode='rgb',\n",
    "                                                    class_mode='categorical', \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    shuffle=True,\n",
    "                                                    seed=38)\n",
    "    # val data flow inititalized #\n",
    "    valgen = test_im_datagen.flow_from_directory(val_path, \n",
    "                                                 target_size=target_size, \n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='categorical', \n",
    "                                                 batch_size=batch_size, \n",
    "                                                 shuffle=True,\n",
    "                                                 seed=38)\n",
    "    # test data flow inititalized #\n",
    "    testgen = test_im_datagen.flow_from_directory(test_path, \n",
    "                                                  target_size=target_size, \n",
    "                                                  color_mode='rgb',\n",
    "                                                  class_mode='categorical', \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  shuffle=True,\n",
    "                                                  seed=38)\n",
    "    \n",
    "    return traingen, valgen, testgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clock():\n",
    "    ''' A simple clock class that prints or hands back the elapsed time between \n",
    "        start and stop calls in a human friendly format. '''\n",
    "    import datetime\n",
    "    def __init__(self):\n",
    "        self.running = False\n",
    "        self.start_time = None\n",
    "        self.stop_time = None\n",
    "        self.elapsed = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.running = True            \n",
    "        self.start_time = datetime.datetime.now()\n",
    "        \n",
    "    def stop(self, stdout=True, handback=False):\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "            self.end_time = datetime.datetime.now()\n",
    "            self.delta = str(self.end_time - self.start_time).split(':')\n",
    "            self.delta[2] = self.delta[2][:2]\n",
    "#             self.elapsed = 'hours:{0[0]}, minutes:{0[1]}, seconds:{0[2]}'\\\n",
    "#                                                             .format(self.delta)\n",
    "            self.elapsed = 'minutes:{0[1]}, seconds:{0[2]}'.format(self.delta)\n",
    "            if stdout:\n",
    "                print(self.elapsed)\n",
    "            if handback:\n",
    "                return self.elapsed\n",
    "            \n",
    "    def __repr__(self):\n",
    "        if self.running:\n",
    "            return 'The clock is running!'\n",
    "        else:\n",
    "            return 'The clock is not running.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "_= gc.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "target_size = (224,224)\n",
    "classes_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [2, 4]\n",
    "filters = [32, 128]\n",
    "kernel_size = [3, 7]    # conv window size\n",
    "pool_size = [2]\n",
    "dense_hidden_layers = [0, 1]\n",
    "optimizer = ['adam', 'rmsprop']\n",
    "lr = [.001, .01]\n",
    "do_rate = [0., .5]\n",
    "batch_normalize = [True, False]\n",
    "batch_size = [64, 128]\n",
    "\n",
    "hyperparams = list(itertools.product(conv_layers, filters, kernel_size, pool_size, \n",
    "                                     dense_hidden_layers, optimizer, lr, do_rate, \n",
    "                                     batch_normalize, batch_size))\n",
    "param_names = \\\n",
    "'''conv_layers, filters, kernel_size, pool_size, dense_hidden_layers, optimizer, \\\n",
    "lr, do_rate, batch_normalize, batch_size'''.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(name, hparams):\n",
    "    ''' Recieves a name for the model and the hyperparameters to create it and returns the \n",
    "        compiled model. '''\n",
    "    # optimizer #\n",
    "    if hparams['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=hparams['lr'])\n",
    "    elif hparams['optimizer'] == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=hparams['lr'], momentum=.9)\n",
    "    # bias vector #    \n",
    "    if hparams['batch_normalize'] == True:\n",
    "        use_bias = False\n",
    "    else:\n",
    "        use_bias = True \n",
    "\n",
    "    # *******  input block  ******* #\n",
    "    input_ = Input(shape=(*target_size, 3), name='input')\n",
    "\n",
    "    # *******  conv blocks  ******* #\n",
    "    # conv block 1 #\n",
    "    x = Conv2D(filters=hparams['filters'], kernel_size=hparams['kernel_size'], strides=1, \n",
    "               padding='same', use_bias=use_bias, activation='relu')(input_)\n",
    "    if hparams['batch_normalize'] == True:\n",
    "            x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=hparams['pool_size'], strides=hparams['pool_size'])(x)\n",
    "    # conv blocks 2 --> #\n",
    "    for i in range(1, hparams['conv_layers']):\n",
    "        x = Conv2D(filters=hparams['filters'], kernel_size=hparams['kernel_size'], strides=1, \n",
    "               padding='same', use_bias=use_bias, activation='relu')(x)\n",
    "        if hparams['batch_normalize'] == True:\n",
    "                x = BatchNormalization()(x)\n",
    "        x = MaxPool2D(pool_size=hparams['pool_size'], strides=hparams['pool_size'])(x)\n",
    "    \n",
    "    # *******  dense blocks  ******* #\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(rate=hparams['do_rate'])(x)\n",
    "    # dense blocks 1 --> #\n",
    "    for i in range(0, hparams['dense_hidden_layers']):\n",
    "        x = Dense(units=hparams['filters'], activation='relu', use_bias=use_bias)(x)\n",
    "        if hparams['batch_normalize'] == True:\n",
    "                x = BatchNormalization()(x)       \n",
    "    # output #\n",
    "    output_ = Dense(units=classes_dim, activation='softmax', name='output')(x)\n",
    "\n",
    "    \n",
    "    # *******  model  ******* #\n",
    "    m = Model(inputs=[input_], outputs=[output_], name=name)\n",
    "\n",
    "    # *******  compile  ******* #\n",
    "    m.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print(m.summary())\n",
    "    \n",
    "    return m    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_test_model(m:tf.keras.Model, fname, hparams, epochs):\n",
    "    ''' Trains the given model, evaluates it on the test data, and logs the results with the \n",
    "    tensorboard callback in the fname location. Returns the history dict and the test accuracy. '''\n",
    "    #***********#   callbacks   #***********#\n",
    "    # cb for logging the metrics\n",
    "    tb_callback = keras.callbacks.TensorBoard(log_dir=fname, histogram_freq=1, \n",
    "                                              write_images=True)\n",
    "    # cb for logging the parameters\n",
    "    hp_callback = hp.KerasCallback(fname, hparams)\n",
    "    # cb for early stopping\n",
    "    earlystop_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=.01, \n",
    "                                                       baseline=.25, patience=5)\n",
    "    \n",
    "    #***********#   data gens   #***********#\n",
    "    # data #\n",
    "    traingen, valgen, testgen = get_data_gens(hparams['batch_size'])\n",
    "    \n",
    "    #***********#   fit and evaluate   #***********#\n",
    "    # fit #\n",
    "    h = m.fit(traingen, \n",
    "              steps_per_epoch=(traingen.n // traingen.batch_size),\n",
    "              validation_data=valgen, \n",
    "              validation_steps=(valgen.n // valgen.batch_size),\n",
    "              epochs=epochs, \n",
    "              callbacks=[tb_callback, hp_callback, earlystop_callback])\n",
    "\n",
    "    # evaluate on test data #\n",
    "    test_loss, test_acc = m.evaluate(testgen)\n",
    "\n",
    "    return h.history, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle_bm_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_bm_lists(bm_ls_mapper):\n",
    "    for path,ls in bm_ls_mapper.items():\n",
    "        with open(pjoin(bm_lists_path, path), 'wb') as file: \n",
    "            pickle.dump(ls, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_benchmarking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmarking():\n",
    "    ''' For the designated indices, the hyperparameter space is explored. All relevant benchmarks\n",
    "        are stored in lists and pickled in the current directory for safe keeping. '''\n",
    "    # maintenance #\n",
    "    keras.backend.clear_session() \n",
    "    gc.collect()\n",
    "    !rm -rf tb_log_path\n",
    "    # benchark lists #\n",
    "    history_ls, test_acc_ls, time_ls, param_ls = [], [], [], []\n",
    "    # benchmark lists fname mapper\n",
    "    bm_ls_mapper = {'history_ls.pkl':history_ls, 'test_acc_ls.pkl':test_acc_ls, \n",
    "                    'time_ls.pkl':time_ls, 'param_ls.pkl':param_ls}\n",
    "    # this is where distribution across multiple instances can be orchestrated #\n",
    "    start_idx, end_idx = 0, len(hyperparams)\n",
    "    \n",
    "    #******************************** run it ********************************#\n",
    "    for run in range(start_idx, end_idx):\n",
    "        params = hyperparams[run]\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        print(f'\\n\\nRUN {run}\\n{param_dict}')\n",
    "\n",
    "        # create model #\n",
    "        m = create_model(str(run), param_dict)\n",
    "\n",
    "        # train and test the model #\n",
    "        clock.start()\n",
    "        h, test_acc = train_test_model(m, \n",
    "                                       os.path.join(tb_log_path, 'run_', str(run)), \n",
    "                                       param_dict,\n",
    "                                       epochs)\n",
    "        clock.stop() \n",
    "        \n",
    "        # append current benchmarks to appropriate bm list\n",
    "        param_ls.append(param_dict)        \n",
    "        history_ls.append(h)\n",
    "        test_acc_ls.append(test_acc)\n",
    "        time_ls.append(clock.elapsed)\n",
    "        \n",
    "        # save lists #\n",
    "        if run % 10 == 0:\n",
    "            pickle_bm_lists(bm_ls_mapper)\n",
    "\n",
    "    # save lists #\n",
    "    pickle_bm_lists(bm_ls_mapper)\n",
    "    \n",
    "    return 'finished' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runtime_clock = Clock()\n",
    "runtime_clock.start()\n",
    "\n",
    "### ******************* ###\n",
    "run_benchmarking()\n",
    "### ******************* ###\n",
    "\n",
    "runtime_clock.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to examine the results!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
